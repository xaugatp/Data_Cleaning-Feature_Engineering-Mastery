# ğŸš€ 12-Week Data Cleaning & Feature Engineering Mastery Guide

> **From raw data â†’ clean dataset â†’ powerful features â†’ reproducible pipelines.**  
> A hands-on, portfolio-ready roadmap based on *Python for Data Analysis (3e)*, extended to 12 weeks with testing, leakage checks, geospatial/time series options, and automation/MLOps.

---

## ğŸ“… Program Overview

| Week | Theme | Status |
|------|-------|--------|
| 1 | ğŸ§¹ Data Familiarity & Baseline Cleaning | [ ] |
| 2 | â“ Missing Values Mastery | [ ] |
| 3 | ğŸš¦ Outliers, Anomalies & Transformations | [ ] |
| 4 | ğŸ·ï¸ Categorical Encoding & Cardinality Control | [ ] |
| 5 | â• Numerical Feature Creation | [ ] |
| 6 | â³ Dates, Time Features & Text Basics | [ ] |
| 7 | ğŸ“ Scaling, Normalization & Pipelines | [ ] |
| 8 | ğŸ”— Data Integration, Joins & Reshaping | [ ] |
| 9 | ğŸ¯ Feature Selection & Dimensionality Reduction | [ ] |
| 10 | ğŸ§ª Data Quality Testing & Automation | [ ] |
| 11 | ğŸ› ï¸ Domain Tracks (TS/NLP/Geo) | [ ] |
| 12 | ğŸ† Capstone: End-to-End Pipeline | [ ] |

*(Replace `[ ]` with `[x]` as you complete each week!)*

---

## ğŸ—‚ï¸ How to Use This Guide
1. **Click the week title** to expand details.  
2. Follow **Objectives** â†’ **Deliverables** â†’ **Mini-project** â†’ **Short course**.  
3. Track progress in the table above.  
4. **Celebrate** after every week â€” data cleaning is a *superpower*. ğŸ¦¸â€â™€ï¸

---

<details>
<summary>ğŸ“… <strong>Week 1 â€“ Data Familiarity & Baseline Cleaning</strong></summary>

### ğŸ¯ Objectives
- Audit schema, dtypes, and value ranges
- Unify naming conventions
- Remove duplicates
- Build a data dictionary

### ğŸ“¦ Deliverables
- `data_dictionary.md`
- Column naming map
- Type-fix script
- Initial EDA/profile report

### ğŸ›  Mini-project
> Audit the Titanic dataset for naming consistency, duplicates, and dtype fixes.

### ğŸ“ Short course
[Kaggle â€“ Pandas](https://www.kaggle.com/learn/pandas)

ğŸ“– **PfDA 3e**: Ch5 Â§5.1â€“5.2, Ch6 Â§6.1, Ch7 Â§7.2, Ch3.3
</details>

---

<details>
<summary>ğŸ“… <strong>Week 2 â€“ Missing Values Mastery</strong></summary>

### ğŸ¯ Objectives
- Quantify missingness
- Visualize patterns of NA
- Choose imputation strategy per feature (drop, constant, KNN, MICE)

### ğŸ“¦ Deliverables
- Missingness report
- Imputation plan
- Before/after data quality table

### ğŸ›  Mini-project
> Use Missingno on the Housing dataset; test 3 imputation methods and compare downstream model impact.

### ğŸ“ Short course
[DataCamp â€“ Handling Missing Data](https://www.datacamp.com/courses/handling-missing-data-in-python)

ğŸ“– **PfDA 3e**: Ch7 Â§7.1, Ch6 Â§6.1
</details>

---

<details>
<summary>ğŸ“… <strong>Week 3 â€“ Outliers, Anomalies & Transformations</strong></summary>

### ğŸ¯ Objectives
- Detect outliers (IQR, Z-score, robust methods)
- Decide treat vs keep using domain context
- Apply transformations (e.g., Yeo-Johnson/Box-Cox)

### ğŸ“¦ Deliverables
- Outlier policy
- Transformation notebook
- Before/after distribution plots

### ğŸ›  Mini-project
> Apply IQR capping on Boston Housing; compare RMSE before vs after.

### ğŸ“ Short course
[Kaggle â€“ Data Cleaning](https://www.kaggle.com/learn/data-cleaning)

ğŸ“– **PfDA 3e**: Ch7 Â§7.2, Ch4 Â§4.3â€“4.4, Appendix A
</details>

---

<!-- Repeat for weeks 4â€“12 in the same expandable format -->

---

## ğŸ§  Key Notes & Pro Tips
ğŸ’¡ **Avoid leakage:**  
- Always fit transformers only on training folds.  
- For target encoding, wrap inside CV folds or use encoders with built-in CV.

ğŸ” **Data QA:**  
- Run schema, bounds, null %, and uniqueness tests before merges and at outputs.

ğŸ— **Join hygiene:**  
- Track match rates and orphan rows with anti-joins.

ğŸ“¦ **Persistence:**  
- Save artifacts with `joblib`, version datasets with DVC.

---

## ğŸ“œ License
MIT â€” free to learn, share, and adapt.

---

## ğŸ“£ Share Your Progress
If you complete a week, share your mini-project on GitHub with the tag:  
